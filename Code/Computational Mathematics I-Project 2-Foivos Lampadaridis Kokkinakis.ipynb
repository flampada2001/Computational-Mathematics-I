{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fde95fdc-c697-4db1-a02c-333d20c84956",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import numpy as np\n",
    "import sympy\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09fb80c1-bfd7-4c6f-9e6d-fe7955ab8784",
   "metadata": {},
   "source": [
    "# Computational Mathematics I, Project 2\n",
    "\n",
    "# Author: Foivos Lampadaridis Kokkinakis\n",
    "\n",
    "## Objective : Applying LU Decomposition for matrix inversion and the Power Method for finding the largest eigenvalue and eigenvector of a symmetric, invertible 5 × 5 matrix. Use the following matrix for your analysis:\n",
    "\n",
    "$$A=\\begin{bmatrix} 4 & 1 &2 &3 &5 \\\\ 1 & 3 & 1 & 4 & 2 \\\\ 2 & 1 & 5 & 2 & 3 \\\\ 3 & 4 &2 &4 &1 \\\\ 5 & 2 &3 &1 &5\\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3184c244-3f0f-4c01-8b0c-793fe7fe79f2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Task 1: LU Decomposition and matrix inversion\n",
    "\n",
    "### Theory\n",
    "\n",
    "Matrices have many applications to mathematics and physics alike. Let's assume we heve the $(n \\times n)$ matrix $A$. One of the most commonly used calculations conserning a matrix $A$ is finding it's inverse $A^{-1}$. __We say that $A^{-1}$ is the inverse of $A$ only if__:\n",
    "\n",
    "$$\\boxed{A A^{-1}= A^{-1} A=I_{nxn}}$$\n",
    "where $I_{nxn}$ is the nxn identity matrix which has diagonal elements are equal to 1 while all its other elements are 0.\n",
    "\n",
    "\n",
    "Unlike scalars (apart from zero) matrices aren't always invertable so we have to be careful. __Before attempting to calculate $A^{-1}$ we first must make sure that it exists, which means that for A it must be true that__:\n",
    "$$\\boxed{det(A)\\neq 0}$$\n",
    "\n",
    "After calculating $det(A)$ the most common way to compute $A^{-1}$ is by applying the formula: $A^{-1}= \\frac{1}{det(A)} Adj(A)$, where $Adj(A)$ is a matrix made of $n$ determinants of $(n-1) \\times (n-1)$ dimensions that need to be calculated. From the definition of the determinant it's clear that one would have to make a very large number of computations to calculate one determinant, since a $(n \\times n)$ determinant is made from $n$ ,$(n-1) \\times (n-1)$ determinants, especially if n is large. To be more specific, the order of computations one has to make to compute a $(n \\times n)$ dereminant is $O(n!)!$ __cite__. This makes the application of the formula swown above, very time consuming and cumputationally costly,which in return makes it clear that we must approach the calculation of $A^{-1}$ differently.\n",
    "\n",
    "From matrix multiplication we know that if two matrices $A$ and $B$ are multiplied and they result is a matrix $C$, the $i$th line of the A matrix and the $j$th column of the B matrix are going to be used to calculate the $c_{i,j}$ element of the $C$ matrix. __Let's assume we are trying to solve a system of $n$ linear equations $A \\times x = b$__ where $A$ is the coefficient matrix with dinmensins $(n \\times n)$, $x$ is the column  matrix of the variables with dinmensins $(n \\times 1)$ and b is the is the column matrix of the constants with dinmensins $(n \\times 1)$. __Now let's assume that the mth element of $b$, $b_m$ is its only non-zero element and it's equal to one__.\n",
    "\n",
    "__If we take into acount that the result of mutiptipling $A$ with its inverse matrix $A^{-1}$ we get the identi matrix $I_{nxn}$ and how matrix multiplication works as explained earlier, we can clearly see that if the only non zero element of b is $b_m$ then the solution of our system must be such that the matrix $x$ contains exactly the elements of the mth column of $A^{-1}$. This means that a calculation of the inverse matrix of a $(n \\times n)$ matrix $A$ is reduced to the calculation of the solution of $n$ number of linnear systems each containing $n$ linear equations.__ By solving each one of these $n$ linnear systems we will calculate one column of $A^{-1}$. __The only differce between these $n$ linnear systems of equations, is going to be the the matrix $b$, since the position of $b_m$ determins the columb of $A^{-1}$ that's been calculated.__\n",
    "\n",
    "Let's assume that we're trying to solve the linnear system:\n",
    "\n",
    "$$A \\times x=b$$\n",
    "\n",
    "\n",
    "__The most commonly used method of solving a number of systems where their only difference is the matrix of constants $b$, is L-U dicomposition.__ L-U dicomposition referst to the process of solving a linner system of equations by __fistly calculating the elements of a lower dioginal matrix $L$ and an upper diagonal matrix $U$, such as: $$\\boxed{L \\times U =A}$$ After that the Matrices $L$ and $U$ are used to solve the system__. One can assume different thing about the maxtrices $L$ and $U$ which will influence the formulas we'd have to use. __Here we chose to work with Crout decomposition, which assumes that the diagonal elemets of $U$ are equal to one.(cite_eng)__\n",
    "\n",
    "__Step 1: Dicomposition)__ To explain how we can calculate the elements of $L$ and $U$ let's assume that $A$ is a $(3 \\times 3)$ matrix. Then:: $A=L\\times U \\Rightarrow A=\\begin{bmatrix} a_{11} & a_{12} & a_{13} \\\\ a_{21} & a_{22} & a_{23} \\\\ a_{31} & a_{32} & a_{33} \\end{bmatrix}=\\begin{bmatrix} l_{11} & 0 & 0 \\\\ l_{21} & l_{22} & 0 \\\\ l_{31} & l_{32} & l_{33} \\end{bmatrix}*\\begin{bmatrix} 1 & u_{12} & u_{13} \\\\ 0 & 1 & u_{23} \\\\ 0 & 0 & 1 \\end{bmatrix}$ Multiplying the elements  $L$'s rows with the first column of  $U$ we get: $l_{11}=a_{11} \\ l_{21}=a_{21} \\ l_{31}=a_{31}$. Knowing the first column of $L$, we multiply its elements with those of the 2nd and 3rd culumb of  $U$ we get: $l_{11}u_{12}=a_{12} \\Rightarrow u_{12}=\\dfrac{a_{12}}{l_{11}}$ και $l_{11}u_{13}=a_{13} \\Rightarrow u_{13}=\\dfrac{a_{13}}{l_{11}}$. We now know the first column of $U$. Using each elemants one would then calculate the elements of the 2nd row of $L$ and then the elemaents of the 2nd column of $U$ and so on. This process is repeated unil all elements of $L$ and $U$ are calculated. Luckily for us there are general formulas for the calculation of these elements which makes the L-U decomposition quite easy to program. __If $A$ is a $(n \\times n)$ matrix the elements of $L$ and $U$ are (cite_eng):__\n",
    "\n",
    "$$\\boxed{l_{ij}=a_{ij}-\\sum_{k=1}^{j-1} l_{ik}u_{kj} \\text{ for } j \\leq i \\text{ and } i=1,2, \\ldots,n} \\boxed{u_{ji}=\\dfrac{a_{ji}-\\sum_{k=1}^{j-1}l_{jk}u_{ki}}{l_{jj}} \\text{ for } j \\leq i \\text{ and } j=2,3, \\ldots,n}$$\n",
    "\n",
    "__where in thet special cace that $j=1$ we have that (cite_eng) $\\boxed{l_{i1}=a_{i1}}$ and when $i=1$ we have that $\\boxed{l_{1j}=\\dfrac{a_{i1}}{l_{11}}}$.\n",
    "What's important is that once $L$ and $U$ have been calculated, if we want to find $A^{-1}$ we don't have to repeated this step again.__\n",
    "\n",
    "__Step 2: Solving the System)__ Now that we have calculated $L$ and $U$, we can write our sustem of linnear equations as: $$\\boxed{L\\times U \\times x=b}$$ If we name $U \\times x=b'$ we have that:\n",
    "\n",
    "$$L \\times b'=b$$\n",
    "\n",
    "which is a linnear system of equations where the matrix of coefficants is now $L$ and the variable metrix is $b'$. __Since $L$ is a low diagonal matrix in reality this system is already solved meaning we cand directly calculate the elements of $b'$ using the front substitution algorithm__. This algorithm is called front substutution since when we apply it, the first variable $b'_1$ is calculated first and then $b_2$ and so on. We can show that:\n",
    "\n",
    "$$ \\boxed{b'_1=\\dfrac{b_1}{l_{11}} \\text{ and } b'_i=\\dfrac{b_i-\\sum_{k=1}^{i-1}l_{ik}b'_k}{l_{ii}},i=2,3,\\ldots,n}$$\n",
    "\n",
    "Now that we know the elements of the $b'$ matrix we can solve the system of our linner equations $U \\times x=b'$. __Since $U$ is a upper diagonal matrix in reality this system is already solved meaning we cand directly calculate the elements of $x$ using the front substitution algorithm__. This algorithm is called backwards substutution since when we apply it, the last variable $x_n$ is calculated first and then $b_{n-1}$ and so on. We can show that:\n",
    "\n",
    "$$ \\boxed{x_n=\\dfrac{b'_n}{u_{11}} \\text{ and } x_i=\\dfrac{b'_i-\\sum_{k=i+1}^{n}u_{ik}x_k}{u_{ii}},i=n-1,n-2,\\ldots,1}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5a3ba9-62df-4c99-ad29-b2f8b544b66e",
   "metadata": {},
   "source": [
    "## Methodology\n",
    "\n",
    "Our application revolves aroun the $(5 \\times 5)$ matrix $A=\\begin{bmatrix} 4 & 1 &2 &3 &5 \\\\ 1 & 3 & 1 & 4 & 2 \\\\ 2 & 1 & 5 & 2 & 3 \\\\ 3 & 4 &2 &4 &1 \\\\ 5 & 2 &3 &1 &5\\end{bmatrix}$. Let's save matrix A in our program so we can perform calculation with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1631a023-116e-4834-a939-379dcb8fe27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "A=np.array([[4, 1, 2, 3, 5,],[1,3,1,4,2],[2,1,5,2,3],[3,4,2,4,1],[5,2,3,1,5]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6e73c4-ac30-4e70-a7d0-9da2706f12fe",
   "metadata": {},
   "source": [
    "__Before iven attemting to calculate the inversse matrix of $A$, we should forst chech if $A$ is invertible. To do so we will use the criterion that was mentioned in the theory section: $det(A)\\neq 0$.__ Using a special function, we calculate the determinant of A, and we save the result in a variable named det_A. We then check if $A$ is invertible or not and we print the appropiate messege/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2436eb0f-d5e8-4d29-aa0f-970e4bda9d7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The determinant of your matrix is equal to: -431.9999999999998 and it's inversible!!\n"
     ]
    }
   ],
   "source": [
    "det_A=np.linalg.det(A)\n",
    "\n",
    "if det_A!=0:\n",
    "    print(\"The determinant of your matrix is equal to:\", det_A,\"and it's inversible!!\")\n",
    "else:\n",
    "    print(\"The determinant of your matrix is equal to:\", det_A,\"and it's not inversible!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd85425-5492-43de-b3b8-5eaa3fad48fb",
   "metadata": {},
   "source": [
    "__As we can see $A$ is invertible so we can attempt to calculate it!__ \n",
    "\n",
    "__Our first step when using the L-U decomposition method is calculating the lower diagonal matrix $L$ and the upper diagonal matrix $U$, based on our matrix $A$.__ To do so we have created an appropriate function named LU_decomposition. This function will take a square matrix $A$ as an input and it will return:\n",
    "\n",
    "__1)__ The lower diagonal matrix $L$ using the appropriate formulas stated in the theory section.\n",
    "\n",
    "__2)__ The upper diagonal matrix $U$ using the appropriate formulas stated in the theory section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d2ceebe-24cf-4f14-a94c-ce9480055d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our function has as input a square matrix A\n",
    "def LU_decomposition(A):\n",
    "    \n",
    "    n=len(A)#The dimensions of A\n",
    "    \n",
    "    L=np.zeros((n,n)) #L is normalized as the 0(nxn) matrix\n",
    "    \n",
    "    U=np.eye(n) #U is normalized as the identity matrix of n dimensions since its diagonal elements are equal to 1\n",
    "    \n",
    "    \n",
    "    for i in range(0,n): #i from 0 to n-1 The bigger index. this means that for the L matrix, it's the row index while for U it's the column index.\n",
    "        \n",
    "        for j in range(0,i+1): # for j from 0 to i. The smaller index. this means that for the L matrix, it's the column index while for U it's the row index.\n",
    "            \n",
    "            #The sums that are used in the formula are normalized\n",
    "            sumL=0\n",
    "            sumU=0\n",
    "\n",
    "            if j==0:#Special case: (The first column of L) \n",
    "                L[i][0]=A[i][0] #The formula that gives us the first column of L (it's equal to the first column of A)\n",
    "            \n",
    "            \n",
    "            if i==0: #Special case (first row of U) \n",
    "                U[0][j]=A[0][j]/L[0][0] #The formula that gives us the first row of U\n",
    "             \n",
    "            \n",
    "            #The sums used in the formulas are calculated for each value of j (sums up to j-1)\n",
    "            for k in range(j):\n",
    "                sumL=sumL+L[i][k]*U[k][j]\n",
    "                sumU=sumU+L[j][k]*U[k][i]\n",
    " \n",
    "            #The formulas that give us the elements of L and U when we're not in a special case\n",
    "            L[i][j]=A[i][j]-sumL\n",
    "            U[j][i]=(A[j][i]-sumU)/L[j][j]\n",
    "            \n",
    "            \n",
    "    return L, U #We return\n",
    "    #1) The Matrix L\n",
    "    #2) The Matrix U"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d2502e-72df-4ea0-8615-59b5e6a3632b",
   "metadata": {},
   "source": [
    "Since we have our function we will use it in our specific problem. Here the matrices we want to calculate $L$ and $U$ out of is $A$. Using L-U decomposition, we must save all its outputs in the proper variables:\n",
    "\n",
    "__1)__ The lower diagonal matrix $L$ is saved in the variable name L_A.\n",
    "\n",
    "__2)__ The upper diagonal matrix $U$ is saved in the variable name U_A."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b8525bae-fa6d-44b2-8f50-c2617acd48b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "L_A ,U_A =LU_decomposition(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba8ce93-a872-46a6-863f-abde78363990",
   "metadata": {},
   "source": [
    "Now that we have calculated the matrices L_A and U_A we don't have to do that again!\n",
    "\n",
    "__We've created a function named LU_Inverse that takes as input the two diagonal matrices named $L$ and $U$ coming from the L-U decomposition of $A$ and tries to calculate $A^{-1}$__.  Its first step is to normalize the inverse of A as a matrix that's the same dimension as $L$ and $U$ filled with zeros. Then it tries to calculate  $A^{-1}$ column by column. \n",
    "\n",
    "Let's assume the ith column of $A^{-1}$ is being calculated. Its first step is to set the ith element of the matrix of constants $b_i=1$ while it sets all its other elements $b_{\\neq i}=0$. This way, the calculation of $A^{-1}$'s' ith column is reduced to solving this system: $L \\times U \\times x=b$.\n",
    "\n",
    "Our next step is to calculate the $b'$ matrix using the element of matrix $L$ and the forward substitution formulas that are mentioned in the theory section. Finally knowing $b'$, using the elements of matrix $U$ and the backward substitution formulas that are mentioned in the theory section, we can calculate the $x$ matrix, which is saved in the ith column of $A^{-1}$. This process is repeated for all columns of $A^{-1}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa176f3f-66c5-425a-8a36-b0ea94145890",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LU_Inverse(L,U):\n",
    "    \n",
    "    n=len(L)#The dimensions of L and thus A\n",
    "    \n",
    "    A_inv=np.zeros((n,n)) #We normalize the inverse of A as the 0(nxn) matrix\n",
    "   \n",
    "    for c in range(n): #for every column of the Matrix A (or L or U)\n",
    "        \n",
    "        b_prime=np.zeros((n,1))#We normalize the b' matrix as the 0(nx1) matrix\n",
    "        b=np.zeros((n,1)) #We normalize the b matrix as the 0(nx1) matrix\n",
    "        \n",
    "        x=np.zeros((n,1)) #X is normalized as 0(nx1) matrix where X[c] will give us the (c+1)th column of A inverse\n",
    "        \n",
    "        b[c]=1 #We set b[c]=1 so we can calculate the (c+1)-th columb of A inverse\n",
    "        \n",
    "        #SOLVING L*b'=b (forward substitution)\n",
    "        for i in range(0,n): # for i from 0 to n-1 (i is an index so, in reality, it's from the 1st to the nth element)\n",
    "            \n",
    "            if i==0:#We check if we are working on the 1st element of b' (0 index) \n",
    "                b_prime[i]=b[0]/L[0][0]# we calculate b'[i] using the appropriate forwards substitution formula\n",
    "            \n",
    "            #if we are working on the other elements of b' we calculate b'[i] using the forward substitution\n",
    "            else:\n",
    "                sum_b_prime=0 #We normalize the sum used in the calculation of b'[i] as 0\n",
    "\n",
    "                #for k from 0 to i-1 (k is the limits of the sum used in the forward substitution formula)\n",
    "                for k in range(i):\n",
    "                    sum_b_prime=sum_b_prime+L[i][k]*b_prime[k] #We calculate the sum we need for the calculation of b'[i]\n",
    "                \n",
    "                b_prime[i]=(b[i]-sum_b_prime)/L[i][i] # we calculate b'[i] using the appropriate forwards substitution formula\n",
    "            \n",
    "        \n",
    "        #SOLVING U*X=b' (backward substitution)\n",
    "        for j in range(n-1,-1,-1): #for j from n-1 to 0 (j is an index so, in reality, it's from the nth  to the 1st element) / (using j we will calculate x[j])\n",
    "                \n",
    "           \n",
    "            if j==n-1: #if we are working on the nth element of x ((n-1)th element of the list) we calculate it using the right formula\n",
    "                x[j]=b_prime[n-1]/U[n-1][n-1]\n",
    "            \n",
    "            else:#if we are working on the other elements of x we calculate x[j] using the backward substitution formula.\n",
    "                \n",
    "                sum_x=0 #We normalize the sum used in the calculation of x[j] as 0\n",
    "                \n",
    "                for k in range(j+1,n): #for k from 0 to i-1 (k is the limits of the sum used in the formulas)\n",
    "                    sum_x=sum_x+U[j][k]*x[k] #We calculate the sum we need for the calculation of x[j]\n",
    "                    \n",
    "                  \n",
    "                x[j]=(b_prime[j]-sum_x)/(U[j][j]) # We apply the backwards substitution formula to calculate the element x[j] \n",
    "                \n",
    "                    \n",
    "        A_inv[:, c] = x.flatten() # We save the elements of x in the cth column of A inverse   \n",
    "            \n",
    "            \n",
    "    return A_inv #We return the inverse matrix of A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9db60a-bc99-4fb7-b63d-c88cd7e79cc3",
   "metadata": {},
   "source": [
    "Since we have our function we will use it in our specific problem. Here the matrix we want to calculate $A^{-1}$.Using L_A and U_A that we got from L-U decomposition, we call our function. We must save all its outputs in the proper variables:\n",
    "\n",
    "__1)__ The inverse of matrix $A$ is saved in a variable named A_inv.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "964c08f6-25db-47d6-ad9d-df837c34d250",
   "metadata": {},
   "outputs": [],
   "source": [
    "A_inv=LU_Inverse(L_A,U_A)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740e23b2-97ec-4181-b4d1-feefa4be9186",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d1bbf29-fa8b-4b28-bc52-0a6d40e99179",
   "metadata": {},
   "source": [
    "__In this section we will present the results we got from our application of th L-U decomposition method in calculating the inverse of matrix $A$__.\n",
    "\n",
    "Firstly we present the two diagonal matrices L_A and U_A that were calculated from our starting matrix $A$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "35552b80-2d5c-4cc5-96a1-70630cc1f01c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The lower diagonal matrix L calculated from A is: \n",
      " [[ 4.          0.          0.          0.          0.        ]\n",
      " [ 1.          2.75        0.          0.          0.        ]\n",
      " [ 2.          0.5         3.90909091  0.          0.        ]\n",
      " [ 3.          3.25       -0.09090909 -2.09302326  0.        ]\n",
      " [ 5.          0.75        0.36363636 -3.62790698  4.8       ]] \n",
      "\n",
      "The upper diagonal matrix U calculated from A is: \n",
      " [[ 1.          0.25        0.5         0.75        1.25      ]\n",
      " [ 0.          1.          0.18181818  1.18181818  0.27272727]\n",
      " [ 0.          0.          1.         -0.02325581  0.09302326]\n",
      " [ 0.          0.          0.          1.          1.73333333]\n",
      " [ 0.          0.          0.          0.          1.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(\"The lower diagonal matrix L calculated from A is: \\n\",L_A,\"\\n\")\n",
    "print(\"The upper diagonal matrix U calculated from A is: \\n\",U_A)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d82e1d0-81ee-4fe9-9b4e-fd1a025c737d",
   "metadata": {},
   "source": [
    "Lastly we present the result from our application of th L-U decomposition method in calculating the inverse of matrix $A$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "77e274a1-adef-4d7b-ae55-3e5f29fd538c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The inverse matrix of A is: \n",
      " [[ 0.24537037 -0.45833333 -0.10185185  0.34259259 -0.06944444]\n",
      " [-0.45833333  0.375      -0.08333333 -0.08333333  0.375     ]\n",
      " [-0.10185185 -0.08333333  0.25925926  0.03703704 -0.02777778]\n",
      " [ 0.34259259 -0.08333333  0.03703704  0.14814815 -0.36111111]\n",
      " [-0.06944444  0.375      -0.02777778 -0.36111111  0.20833333]]\n"
     ]
    }
   ],
   "source": [
    "print(\"The inverse matrix of A is: \\n\", A_inv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58b077f-06c2-420a-9110-9dfe88f02a49",
   "metadata": {},
   "source": [
    " ### Analysis/Discussion\n",
    " \n",
    " In this section we will check the validity of our results and commend on them. Let's begin with testing the validity of our L-U decomposition step. Theoretically we know that if we multiply the two matrices we calculated, L_A with U_A, we should get our initial matrix $A$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a27f6221-4fa5-42c9-9282-b9098f70a57b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The result of multiplying L with U is: \n",
      " [[4. 1. 2. 3. 5.]\n",
      " [1. 3. 1. 4. 2.]\n",
      " [2. 1. 5. 2. 3.]\n",
      " [3. 4. 2. 4. 1.]\n",
      " [5. 2. 3. 1. 5.]] \n",
      "\n",
      "A=\n",
      " [[4 1 2 3 5]\n",
      " [1 3 1 4 2]\n",
      " [2 1 5 2 3]\n",
      " [3 4 2 4 1]\n",
      " [5 2 3 1 5]]\n"
     ]
    }
   ],
   "source": [
    "print(\"The result of multiplying L with U is: \\n\",np.dot(L_A,U_A),\"\\n\\nA=\\n\",A)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b6deba-6d64-4e16-a364-d1273cd9bfc8",
   "metadata": {},
   "source": [
    "What this result tells us is that up to the L-U decomposition step, our calculations have been correct and worked as expected.\n",
    "\n",
    "The final step we must take is to check if the matrix we endend up with is actully $A^{-1}$. As stated in the theory section the product of $A$ with its' inverse should give us the 5 by 5 identity matrix $I_{5 \\times 5}$. To check if this is true we will calculte $A$ with the matrix we ended up with (A_inv):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "24e5a42a-c9af-4f54-95ab-f93cc95264da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The product of A with A_inv is:\n",
      "\n",
      " [[ 1.0000e+00  1.1102e-16  1.0408e-17  2.2204e-16  8.3267e-17]\n",
      " [-5.5511e-17  1.0000e+00 -6.9389e-18  0.0000e+00 -5.5511e-17]\n",
      " [-8.3267e-17  1.1102e-16  1.0000e+00  0.0000e+00  1.3878e-16]\n",
      " [ 3.0531e-16  1.1102e-16  1.0755e-16  1.0000e+00 -2.4980e-16]\n",
      " [ 8.3267e-17  5.5511e-16 -1.0061e-16  0.0000e+00  1.0000e+00]]\n"
     ]
    }
   ],
   "source": [
    "with np.printoptions(precision=4):\n",
    "    print(\"The product of A with A_inv is:\\n\\n\",np.dot(A,A_inv))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb973671-5325-43e8-bc45-81ccbcefde8e",
   "metadata": {},
   "source": [
    "The result we get is indeed not equal to the identity matrix, since some of it's non diagonial elements are close but not equal to zero. __This was expected as the number of calculations our computer has to perform increases, the accuracy of our final result decreases. This is due to the round off error and we can't do anything about it, other than finding another method of calculating $A^{-1}$ which would require less calculations. This means that our algorythm worked as expected and that the L-U decomposition can indeed be used for calculating the inverse of a matrix $A$.__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d1efa9-8f51-46de-8f22-8311c1e86789",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Conclusion\n",
    "\n",
    "In conclution it's clear that that L-U decomposition method is really important when trying to compute the inverse of a matrix $A$. The biggest strength of this method is that a big part of the calculation (computing $L$ and $U$) has to be done one time. After that we are able to use the lower and upper diagonal metrices $L$ and $U$ to compute each column of $A$ cepertly just by altering the constant matrix $b$. This makes the computation of $A^{-1}$ a lot quicker than some more well known methods like the Gauss elimination method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca741c7-15e7-49c6-98cf-9542a466e02c",
   "metadata": {},
   "source": [
    "---------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "464f98a2-00af-4915-8591-ad87955245f3",
   "metadata": {},
   "source": [
    "## Task 2: The power method and the computation of the biggest eigenvalue of a matrix\n",
    "\n",
    "### Theory\n",
    "\n",
    "Another important calculation one may be required to do when working with matrices is to compute its' eigenvalues and the corresponding eigenvectors. In this project we will just focus on the computation of the largest eigenvalue of a given matrix, using the power method.\n",
    "\n",
    "The definition of an eigenvector and eigenvalue is given by the following:If $A$ is a n by n matrij we say that $y^{(i)}$ and $\\lambda_i$ is its eigenvector and eigenvalue if and only if:\n",
    "\n",
    "$$\\boxed{A \\times y^{(i)}=\\lambda_i \\times y^{(i)}}$$\n",
    "\n",
    "Classically the computation of the eigenvalues of a n by n matrix $A$ is given by it's characteristical polynomyal $$det(A-\\lambda*I_{n \\times n})=0$$\n",
    "\n",
    "As we can clearly see, the above requires the computation of an n by n determinant which is very computationally costly since its' compacity is $O(n!)$ __cite__! This makes it clear the eigenvalues of a matrix should be calculated in a different way. Let's assume that the matrix we are working on is $A$ and since it is a n by n matrix, its characteristic polynomial could theoticly be an n-th degree polynomial, meaning it could have n different eigenvalues $\\lambda_i$. If we sort $A$'s from larges to smalest we get: $$|\\lambda_1|>|\\lambda_2|>\\ldots>|\\lambda_n|$$\n",
    "\n",
    "An important note to make here is that the eigenvectors of a n by n matrix $A$, create a n-dimensin orthocanonical system. This means that every other n-dimension vector, can be written as a linnear combination of $A$'s eigenvectors. In other words we can write any vector $x$ as:\n",
    "$$x=a_1y^{(1)}+a_2y^{(2)}+\\ldots+a_ny^{(n)}$$\n",
    "where $y^{(i)}$ are $A$'s eigenvectors that correspind to the eigenvalue $\\lambda_i$. Since every vector can be written as a linnear combination of $A$'s eigenvectors if we take the product of $A$ with a random vector $x$ we will have:$$x^{(1)}=A \\times x=A \\times \\sum_{i=1}^{n}a_i y^{(i)}= \\lambda_1a_1y^{(2)}+\\lambda_2a_2y^{(i)}\\ldots+\\lambda_na_ny^{(n)}$$\n",
    "\n",
    "If we keep mutiplying $x^{(1)}$ by $A$ (let's say $k$ times) we will get: $$x^{(k)}=A \\times x^{(k-1)}=\\lambda_1^k\\left(a_1y^{(1)}+\\left(\\frac{\\lambda_2}{\\lambda_1}\\right)^k a_2 y^{(2)}+\\ldots+\\left(\\frac{\\lambda_n}{\\lambda_1}\\right)^k a_n y^{(n)}\\right) \\approx \\lambda_1^ka_1y^{(1)}$$\n",
    "\n",
    "where the the turms with:$\\frac{\\lambda_i}{\\lambda_1}<1 \\Rightarrow \\left(\\frac{\\lambda_i}{\\lambda_1}\\right)^k \\approx 1$ approch zero as $k$ increases, since $|\\lambda_1|>|\\lambda_i|$.\n",
    "\n",
    "If we devide the elements of $x^{(k)}$ with the elements of $x^{(k-1)}$ we will have: $u=\\left(\\dfrac{x^{(k)}_1}{x^{(k-1)}_1},\\dfrac{x^{(k)}_2}{x^{(k-1)}_2},\\ldots,\\dfrac{x^{(k)}_n}{x^{(k-1)}_n}\\right)=\\left( \\dfrac{\\lambda_1^k a_1y^{(1)}_1}{\\lambda_1^{k-1} a_1y^{(1)}_1},\\dfrac{\\lambda_1^k a_1y^{(1)}_2}{\\lambda_1^{k-1} a_1y^{(1)}_2} \\right)=\\left(\\lambda_1,\\lambda_1,\\ldots,\\lambda_1 \\right)$. In other word the vector $u$ will be a $n$ long vector ,where each one of its' components is an approxiamation of $A$'s largest eigenvalue, $\\lambda_1$\n",
    "\n",
    "The corresponding eigenvector of $\\lambda_1$ will be $x^{(k)}=y^{(1)}$ since when it's mutlyplied by $A$ we will get $Ax^{(k)}=a_1\\lambda_1y^{(1)}=b_ay^{(1)}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f751d1fb-270c-4d40-a1a6-0c7f5bb0c10f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Methodology\n",
    "\n",
    "To impliment the power method we chose to create a funtion named power_method that takes as input:\n",
    "\n",
    "__1)__ The matrix whos eigenvector and eigenvalue we are trying to find named A\n",
    "\n",
    "__2)__ How many signifificant figures of precission the user demands for the value of the eigenvalue named significant_figures_value\n",
    "\n",
    "__4)__ How many signifificant figures of precission the user demands for the value of the eigenvector named significant_figures_vector\n",
    "\n",
    "__3)__ The maximum amound of iteration the user is going to allow, named max_count\n",
    "\n",
    "To begin the implimentation we first have to make an initial guess $x$ for the eigenvector of A's largest eigenvalue $\\lambda_1$. We will always choose the $n$-long unitary vector as our initial guess. Since in our case $A$ is a 5 by 5 matrix $x$, we choose $x_k=(1,1,1,1,1)$. We then multiply $x$ with $A$, and we get x_k_plus_1, which is saved in a list called eig_vec_results __From the theory section we can tell that x_k_plus_1 will be the next best approximation for the eigenvalue we are lokking for__. In each iteration firtsly we normalize the old approximation of the eigenvector then calculate the new one by multyplying the old one with $A$. Then we normalize that and we save it in the proper list. After that we divide the largest element of x_k_plus_1 with the largest element of x_k and we save that on the propper list. That value is going to be our approximation for the eigenvalue for this iteration as explayind from eq. toso. After that the approximation perchantage error  for the eigenvector and eigenvector is calculated and saved in the proper lists. The latter is calculated only from the first element from the eigevector since we assume that every element is going to converge in a similar matter. Finally the eigenvalue and eigenvector we got in this iteration become the old ones and the method is repeated as long as the necessary pressisions aren't met and max amount of steps hasn't been reched.\n",
    "\n",
    "\n",
    "__In each iteration we calculate the eigenvalue acording to the theory, by deviding the largest coefficant of x_k with the largest coeeficant of x_k_plus_1 to achive bigger precission__. This is temporarly saved in a variable named u_k_plus_1 and the stored in a list called u_results. From the value of u in the previous iteration u_k and the values of u in this iteration u_k_plus_1, __we calculate the approximate perchantage error in the value of u and we store it in the list named ea_results. This value will be later used to check if the neccesary precission was achived using the Scarborough formula.__ Finally the approximation of the eigenvector is normalised by deviding its' elements by the larges one and  the new values of u and x (x__k_plus_1,u__k_plus_1) are taken and they are put in the old ones (x_k and u_k) so the method can be repeated if the precission isn't met or the max amount of steps hasn't been reched.\n",
    "\n",
    "In the end our function returns:\n",
    "\n",
    "__1)__ The biggest eigenvalue of matrix A with significant_figures number of significant figures of precision\n",
    "\n",
    "__2)__ The eigenvector that corrispond to the biggest eigen vector\n",
    "\n",
    "__3)__ a list with the approximate percentage error of the eigenvalue, calculated in each iteration.\n",
    "\n",
    "__4)__ a list with the calculated eigenvector in each iteration\n",
    "\n",
    "__5)__ a list with the calculated eigenvalue in each iteration\n",
    "\n",
    "__6)__ the number of iterations it took.\n",
    "\n",
    "The funcion power_method is defyined below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "72bb097f-2dd1-4d71-b432-edb3d58384f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our function takes as input:\n",
    "#1) A: The matrix whose bigger eigenvalue and the corresponding eigenvector the user wants to find\n",
    "#2) significant_figures_value: The number of significant figures of precision the user demands the eigenvalue to have. After this precision is reached, the iteration stops.\n",
    "#3) significant_figures_vector: The number of significant figures of precision the user demands the eigenvector to have. After this precision is reached, the iteration stops.\n",
    "#4) max_count is the upper limit of iterations that are going to be performed. This is more of a safety measure in case the user makes a mistake, or something goes wrong.\n",
    "def power_method(A,significant_figures_value,significant_figures_vector,max_count):\n",
    "     \n",
    "    ea_value=100 #The normalization of the approximate percentage error of the eigenvalue so the method can begin.\n",
    "    ea_vector=100 #The normalization of the approximate percentage error of the eigenvector so the method can begin.\n",
    "    \n",
    "    #The Scarborough Formula. We use it to compute the limit of the percentage approximate error we require to reach the precision that the user asked for for the eigenvalue and vector.\n",
    "    es_value=0.5*10**(2-significant_figures_value)\n",
    "    es_vector=0.5*10**(2-significant_figures_vector)\n",
    "\n",
    "    \n",
    "    n=len(A) #The dimensions of matrix A\n",
    "    \n",
    "    x_k=np.ones((n,1)); #Our initial guess for the eigenvector corresponding to the largest eigenvalue\n",
    "    eig_vec_results=[] #This is where our approximations of the eigenvector in each iteration will be saved\n",
    "    \n",
    "    eig_value_results=[] #This is where the vector u will be saved in each iteration (its elements approximate the largest eigenvalue)\n",
    "    u_k=1 #The normalization of the vector u_k so the loop can be calculated the first time through (this value doesn’t matter)\n",
    "    \n",
    "    ea_results_value=[] # This is where the approximate percentage error of the largest eigenvalue will be saved in each iteration\n",
    "    ea_results_vector=[] # This is where the approximate percentage error of the corresponding eigenvector will be saved in each iteration\n",
    "    \n",
    "    count=0 #The count of the iteration we're on is normalized at 0\n",
    "    while abs(ea_value)>es_value or abs(ea_vector)>es_vector and count<=max_count:#While the approximate error or the eigenvalue or the approximate error or the eigenvector\n",
    "        #is lower than what is given by the Scarborough Formula (the necessary precision isn't reached) and while we are under the maximum number of iterations, the method is applied.\n",
    "    \n",
    "        count=count+1 #We are in the next iteration\n",
    "        \n",
    "        eig_vec_k=(x_k/(np.max(x_k))).flatten() #The old approximation of the eigenvector is normalized and saved\n",
    "        \n",
    "        \n",
    "        x_k_plus_1=np.dot(A,x_k) #The vector x_(k+1)=A*x_k is calculated\n",
    "        eig_vec_k_plus_1=(x_k_plus_1/(np.max(x_k_plus_1))).flatten() #The eigenvector is calculated by dividing the x_k+1 vector by its larger component (normalization) and saved\n",
    "        #in the appropriate list\n",
    "        \n",
    "        \n",
    "        \n",
    "        eig_vec_results.append(eig_vec_k_plus_1) #The vector normalized vector x_(k+1) is saved in the appropiate list\n",
    "        \n",
    "        u_k_plus_1=(np.max(x_k_plus_1)/np.max(x_k)) #We calculate the vector u, whose elements give us an estimate for the eigenvalue\n",
    "        eig_value_results.append(u_k_plus_1) #The vector u is saved in the appropriate list\n",
    "        \n",
    "        ea_value=abs((u_k_plus_1-u_k)/(u_k_plus_1))*100 #We calculate the new value of the approximate percentage error of the eigenvalue by dividing the vectors \n",
    "        #u_(k+1) and u_k by element\n",
    "        ea_results_value.append(ea_value) #We save it in the proper list.\n",
    "        \n",
    "        ea_vector=abs((eig_vec_k_plus_1[0]-eig_vec_k[0])/eig_vec_k_plus_1[0])*100 #We calculate the new value of the approximate percentage error\n",
    "        #of the eigenvector from its' first element\n",
    "        ea_results_vector.append(ea_vector) #We save it in the proper list.\n",
    "        \n",
    "    \n",
    "        \n",
    "        u_k=u_k_plus_1 ##The best new approximation of the vector u, becomes the old one and the method is repeated\n",
    "        x_k=x_k_plus_1 #The new vector x_(k+1) becomes the old one and the method is repeated\n",
    "        \n",
    "        if count==max_count:#If the max amount of iteration was reached\n",
    "            print(\"Warning!!! the maximum number of iterations has been reached! The result isn't as precise as you've asked!\") #Print a warning message to the user\n",
    "            \t\n",
    "    return u_k_plus_1,eig_vec_k_plus_1.flatten(), ea_results_value,ea_results_vector, eig_vec_results,eig_value_results, count #We return\n",
    "    # 1) The biggest eigenvalue of matrix A with significant_figures number of significant figures of precision\n",
    "    # 2) The eigenvector that corresponds to the biggest eigenvector.\n",
    "    # 3) a list with the approximate percentage error of the eigenvalue, calculated in each iteration.\n",
    "    # 4) a list with the approximate percentage error of the eigenvector, calculated in each iteration.\n",
    "    # 5) a list with the calculated eigenvector in each iteration\n",
    "    # 6) a list with the calculated eigenvalue in each iteration\n",
    "    # 7) The number of iterations it took."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a582680b-a446-43c0-a6ab-97cf554d4536",
   "metadata": {},
   "source": [
    "Since we have our function we will use it in our specific problem. Here we want to calculate A's largest eigenvalue and the corresponding eigenvector, we ask for 10 significant figures of precision for both, and we allow for 10000 iterations. We must save all its outputs in the proper variables:\n",
    "\n",
    "__1)__ The biggest eigenvalue of matrix $A$ with 10 significant figures of precision is saved in max_eigenvalue\n",
    "\n",
    "__2)__ The eigenvector that corresponds to the biggest eigenvalue is saved in max_eigenvector\n",
    "\n",
    "__3)__ The list with the approximate percentage error of the eigenvalue, calculated in each iteration is saved in approximate_error_value.\n",
    "\n",
    "__4)__ The list with the approximate percentage error of the eigenvector, calculated in each iteration is saved in approximate_error_vector.\n",
    "\n",
    "__5)__ The list with the calculated eigenvector in each iteration is saved in eigenvector_results\n",
    "\n",
    "__6)__ The list with the calculated eigenvalue in each iteration is saved in eigenvalue_results\n",
    "\n",
    "__7)__ The number of iterations the power method took to reach the desired precision is saved in num_of_iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "246902a0-8d33-4562-971a-7deb2eda4167",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_eigenvalue, max_eigenvector, approximate_error_value, approximate_error_vector ,eigenvector_results, eigenvalue_results, num_of_iterations=power_method(A,10,10,10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "581ba20c-8f46-4e60-ac26-f9e34cf7df05",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e69521-502e-4e5e-8f19-7c72eb292e51",
   "metadata": {},
   "source": [
    "Here we will present the results we got from the power method. First off we will see the largest eigenvalue and eigen value of $A$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "14662399-e0d4-4c46-91a5-fb57800243a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We needed 23 iterations. The biggest eigenvalue of A is: 14.07561378 \n",
      "and the eigenvector is: [0.9445493184 0.6180313193 0.7787901191 0.7804341894 1.          ]\n"
     ]
    }
   ],
   "source": [
    "with np.printoptions(precision=10):\n",
    "    print(\"We needed\", num_of_iterations, \"iterations.\",\n",
    "          \"The biggest eigenvalue of A is:\", \"{:.8f}\".format(max_eigenvalue),\n",
    "          \"\\nand the eigenvector is:\", max_eigenvector.T)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2497ca-edf4-468d-ba4c-e5577b3aeb8f",
   "metadata": {},
   "source": [
    "Now we will create a list, in which we save a natural number up to how many iterations were performed. We will use this list to plot the approximate perchantage error of the eigenvalue and eigenvector. We will also save the expacted value for both in es_exp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d4908fe5-eb98-46e6-ae2b-250533a5aba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "iteration_space_pow=np.linspace(1,num_of_iterations,num_of_iterations)\n",
    "es_exp=0.5*10**(2-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8f015eec-34be-42ac-8428-b924b190761d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Power method\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.hlines(y=es_exp,xmin=min(iteration_space_pow),xmax=max(iteration_space_pow),\n",
    "          color='red',linestyle='--',label='Desired approximate error es') #A line that show the approximate percentage error required for\n",
    "ax.plot(iteration_space_pow[1:],approximate_error_value[1:],label='Error of the largest eigenvalue (%)') #plotting the curve of the total error (%)\n",
    "ax.plot(iteration_space_pow[1:], approximate_error_vector[1:],label='Error of the eigenvector (%)') #plotting the curve of the approximate error (%)\n",
    "#Note: the approximate error makes sense only after two approximations.\n",
    "ax.set(xlabel='Number of iterations', ylabel='Approximate Percentage Error (%)') #Labels of axis\n",
    "ax.set_yscale('log') #Log scale\n",
    "ax.set_title('Convergence of power method') #Title\n",
    "ax.legend() #show legend\n",
    "ax.grid()\n",
    "#plt.savefig(\"figure.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e82a7a-6caf-45bb-a300-086c806f1868",
   "metadata": {},
   "source": [
    "Here we will check if max_eigenvalue and max_eigenvector are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3c157030-34dc-42c7-afe0-ea8d9d13df84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y^{(1)}=A*y^{(1)}/λ_1= [0.94454932 0.61803132 0.77879012 0.78043419 1.        ] \n",
      "where y^{(1)}= [0.94454932 0.61803132 0.77879012 0.78043419 1.        ]\n"
     ]
    }
   ],
   "source": [
    "print(\"y^{(1)}=A*y^{(1)}/λ_1=\",(np.dot(A,max_eigenvector))/max_eigenvalue,\n",
    "      \"\\nwhere y^{(1)}=\",max_eigenvector)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
